{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f9d65d",
   "metadata": {},
   "source": [
    "# Домашнее задание 1\n",
    "## выполнила Быкова Валерия из группы БКЛ-213"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c54fc",
   "metadata": {},
   "source": [
    "Загружаем проверку стиля кода.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c7b105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycodestyle in /Users/valeriabykova/opt/anaconda3/lib/python3.9/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: flake8 in /Users/valeriabykova/opt/anaconda3/lib/python3.9/site-packages (3.9.2)\r\n",
      "Requirement already satisfied: pycodestyle_magic in /Users/valeriabykova/opt/anaconda3/lib/python3.9/site-packages (0.5)\r\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Users/valeriabykova/opt/anaconda3/lib/python3.9/site-packages (from flake8) (0.6.1)\r\n",
      "Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in /Users/valeriabykova/opt/anaconda3/lib/python3.9/site-packages (from flake8) (2.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pycodestyle flake8 pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3aebce",
   "metadata": {},
   "source": [
    " Устанавливаем проверку стиля кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46187a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95ad4f",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ba9e6",
   "metadata": {},
   "source": [
    " Читаем текст \"Детективное агентство Дирка Джентли\" Дугласа Адамса и перезаписываем в файл с кодировкой utf-8, так как на lib.ru не скачивалось с нужной кодировкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201325a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gently_1.txt', encoding='KOI8-R') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open('gently_utf.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4356bf",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fe772",
   "metadata": {},
   "source": [
    "Импортируем **Mystem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b28394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd81fef",
   "metadata": {},
   "source": [
    "Лемматизируем текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cc4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee04b2",
   "metadata": {},
   "source": [
    "Запись лемм в новый файл.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3815b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book_lemmas.txt', 'w', encoding='UTF-8') as f:\n",
    "    for lemma in lemmas:\n",
    "        f.write(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ded86b",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53bbc39",
   "metadata": {},
   "source": [
    "Импорт инструментов **nltk** и **pymorphy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b641c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0e255",
   "metadata": {},
   "source": [
    "Токенизация текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d0d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56adbee8",
   "metadata": {},
   "source": [
    "Создание экземпляра класса для анализа с помощью **pymorphy**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b31b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad923a98",
   "metadata": {},
   "source": [
    "Проходимся по списку слов, получившемуся в результате токенизации, создаем список словарей, содержащих лемму, словоформу и часть речи для каждого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed2bcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_words = []\n",
    "for word in words:\n",
    "    dict_of_parsed_words = dict()\n",
    "    ana = morph.parse(word)\n",
    "    first = ana[0]  # первый разбор\n",
    "    dict_of_parsed_words['lemma'] = first.normal_form\n",
    "    dict_of_parsed_words['word'] = first.word\n",
    "    dict_of_parsed_words['pos'] = first.tag.POS\n",
    "    parsed_words.append(dict_of_parsed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086874d",
   "metadata": {},
   "source": [
    "Пример списка словарей, который будет перенесен в **jsonl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c5a387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lemma': 'башня', 'word': 'башни', 'pos': 'NOUN'},\n",
       " {'lemma': 'силуэт', 'word': 'силуэт', 'pos': 'NOUN'},\n",
       " {'lemma': 'который', 'word': 'которой', 'pos': 'ADJF'},\n",
       " {'lemma': 'одиноко', 'word': 'одиноко', 'pos': 'ADVB'},\n",
       " {'lemma': 'торчать', 'word': 'торчал', 'pos': 'VERB'},\n",
       " {'lemma': 'над', 'word': 'над', 'pos': 'PREP'},\n",
       " {'lemma': 'первородный', 'word': 'первородным', 'pos': 'ADJF'},\n",
       " {'lemma': 'месиво', 'word': 'месивом', 'pos': 'NOUN'},\n",
       " {'lemma': 'грязь', 'word': 'грязи', 'pos': 'NOUN'},\n",
       " {'lemma': 'она', 'word': 'её', 'pos': 'NPRO'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_words[90:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e59ab",
   "metadata": {},
   "source": [
    "Запись в файл формата **jsonl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fc2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pymorphytojsonl.jsonl', 'w') as f:\n",
    "    for i, el in enumerate(parsed_words):\n",
    "        line = json.dumps(el, ensure_ascii=False)\n",
    "        f.write(line)\n",
    "        if i != len(parsed_words) - 1:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbdbd1",
   "metadata": {},
   "source": [
    "## Задание 4. Ответы на вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f447072",
   "metadata": {},
   "source": [
    "### Вопрос 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbd80d",
   "metadata": {},
   "source": [
    "Считаем вхождения каждой части речи в процентах. В тексте есть слова на английском языке, их часть речи - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9914d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8d44fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В этой книге NOUN - 24.20 % от всех слов.\n",
      "В этой книге VERB - 14.05 % от всех слов.\n",
      "В этой книге ADJF - 10.22 % от всех слов.\n",
      "В этой книге CONJ - 10.08 % от всех слов.\n",
      "В этой книге PREP - 9.86 % от всех слов.\n",
      "В этой книге NPRO - 9.54 % от всех слов.\n",
      "В этой книге ADVB - 7.52 % от всех слов.\n",
      "В этой книге PRCL - 5.96 % от всех слов.\n",
      "В этой книге INFN - 3.08 % от всех слов.\n",
      "В этой книге GRND - 1.38 % от всех слов.\n",
      "В этой книге PRTF - 1.19 % от всех слов.\n",
      "В этой книге ADJS - 1.01 % от всех слов.\n",
      "В этой книге PRED - 0.48 % от всех слов.\n",
      "В этой книге NUMR - 0.40 % от всех слов.\n",
      "В этой книге COMP - 0.40 % от всех слов.\n",
      "В этой книге PRTS - 0.32 % от всех слов.\n",
      "В этой книге None - 0.17 % от всех слов.\n",
      "В этой книге INTJ - 0.14 % от всех слов.\n"
     ]
    }
   ],
   "source": [
    "list_of_tags = [word['pos'] for word in parsed_words]\n",
    "pos_amount_list = Counter(list_of_tags).most_common()\n",
    "all_words = len(list_of_tags)\n",
    "for pos_amount in pos_amount_list:\n",
    "    pos_percent = (int(pos_amount[1]) / all_words) * 100\n",
    "    print(f'В этой книге {pos_amount[0]} - {pos_percent:.2f} % от всех слов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288128c",
   "metadata": {},
   "source": [
    "### Вопрос 2\n",
    "Считаем топ-20 по частотности глаголов и наречий. Я не поняла, вместе или по отдельности, поэтому сделала функцию, чтобы рассмотреть оба варианта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83bdf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_popular_words(tokens, tags, n):\n",
    "    lemmas = [word['lemma'] for word in tokens if word['pos'] in tags]\n",
    "    top_n = Counter(lemmas).most_common(n)\n",
    "    for i, (lemma, count) in enumerate(top_n):\n",
    "        print(f'{i + 1}. {lemma}: {count} шт.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc5c20",
   "metadata": {},
   "source": [
    "Топ-20 самых частых глаголов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8fc3784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. быть: 832 шт.\n",
      "2. мочь: 225 шт.\n",
      "3. знать: 156 шт.\n",
      "4. сказать: 155 шт.\n",
      "5. казаться: 112 шт.\n",
      "6. хотеть: 91 шт.\n",
      "7. стать: 90 шт.\n",
      "8. спросить: 83 шт.\n",
      "9. ответить: 68 шт.\n",
      "10. посмотреть: 67 шт.\n",
      "11. говорить: 61 шт.\n",
      "12. видеть: 59 шт.\n",
      "13. понимать: 59 шт.\n",
      "14. сделать: 58 шт.\n",
      "15. заметить: 46 шт.\n",
      "16. оказаться: 44 шт.\n",
      "17. вернуться: 44 шт.\n",
      "18. иметь: 43 шт.\n",
      "19. попытаться: 42 шт.\n",
      "20. остановиться: 41 шт.\n"
     ]
    }
   ],
   "source": [
    "print_popular_words(parsed_words, {'VERB'}, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978bd7d",
   "metadata": {},
   "source": [
    "Топ-20 самых частых наречий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47baa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ещё: 190 шт.\n",
      "2. уже: 141 шт.\n",
      "3. снова: 137 шт.\n",
      "4. наконец: 110 шт.\n",
      "5. только: 104 шт.\n",
      "6. теперь: 99 шт.\n",
      "7. ничего: 97 шт.\n",
      "8. вдруг: 89 шт.\n",
      "9. здесь: 86 шт.\n",
      "10. где: 81 шт.\n",
      "11. очень: 77 шт.\n",
      "12. тут: 72 шт.\n",
      "13. там: 69 шт.\n",
      "14. более: 68 шт.\n",
      "15. затем: 63 шт.\n",
      "16. почему: 62 шт.\n",
      "17. потом: 62 шт.\n",
      "18. несколько: 60 шт.\n",
      "19. совсем: 55 шт.\n",
      "20. пока: 54 шт.\n"
     ]
    }
   ],
   "source": [
    "print_popular_words(parsed_words, {'ADVB'}, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4312f46",
   "metadata": {},
   "source": [
    "Топ-20 самых частых глаголов и наречий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e65d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. быть: 832 шт.\n",
      "2. мочь: 225 шт.\n",
      "3. ещё: 190 шт.\n",
      "4. знать: 156 шт.\n",
      "5. сказать: 155 шт.\n",
      "6. уже: 141 шт.\n",
      "7. снова: 137 шт.\n",
      "8. казаться: 112 шт.\n",
      "9. наконец: 110 шт.\n",
      "10. только: 104 шт.\n",
      "11. теперь: 99 шт.\n",
      "12. ничего: 97 шт.\n",
      "13. хотеть: 91 шт.\n",
      "14. стать: 90 шт.\n",
      "15. вдруг: 89 шт.\n",
      "16. здесь: 86 шт.\n",
      "17. спросить: 83 шт.\n",
      "18. где: 81 шт.\n",
      "19. очень: 77 шт.\n",
      "20. тут: 72 шт.\n"
     ]
    }
   ],
   "source": [
    "print_popular_words(parsed_words, {'ADVB', 'VERB'}, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878f417",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10d068",
   "metadata": {},
   "source": [
    "Импортируем список стоп-слов, так как с ними n-граммы получаются не очень осмысленными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7edbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valeriabykova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "sw = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a5e5c",
   "metadata": {},
   "source": [
    "Создаем список лемм, которые выделил анализатор pymorphy и избавляемся от стоп-слов. Со стоп-словами н-граммы совсем бессмысленные, но с помощью стоп-слов от nltk легче не становится, потому что там, к примеру, есть слово \"эту\", но \"это\" -- нет. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1fcfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_lemmas = [token['lemma'].replace('ё', 'е') for token in parsed_words]\n",
    "filtered = [w for w in pymorphy_lemmas if w not in sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259ae6f",
   "metadata": {},
   "source": [
    "Создаем списки биграмм и триграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12b46a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_bigrams = list(nltk.bigrams(filtered))\n",
    "list_of_trigrams = list(nltk.trigrams(filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69e236",
   "metadata": {},
   "source": [
    "Функция для подсчета и печати топа н-грамм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b9ef159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_x_ngrams(list_of_ngrams, x, n):\n",
    "    top_ngrams = Counter(list_of_ngrams).most_common(x)\n",
    "    print(f'Топ-{x} {n}-грамм:')\n",
    "    for i, (ngram, amount) in enumerate(top_ngrams):\n",
    "        ngram = ' '.join(ngram)\n",
    "        print(f'{i + 1}. {ngram}: {amount} шт.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6c38e",
   "metadata": {},
   "source": [
    "Выводим топ биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f3d488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-25 2-грамм:\n",
      "1. это сделать: 26 шт.\n",
      "2. черта побрать: 25 шт.\n",
      "3. весь лишь: 23 шт.\n",
      "4. гордон уэй: 23 шт.\n",
      "5. сделать это: 22 шт.\n",
      "6. миссис зускинд: 21 шт.\n",
      "7. сказать это: 20 шт.\n",
      "8. спросить ричард: 18 шт.\n",
      "9. пожать плечо: 18 шт.\n",
      "10. гордон уэя: 18 шт.\n",
      "11. положить трубка: 18 шт.\n",
      "12. мисс пирс: 18 шт.\n",
      "13. это время: 16 шт.\n",
      "14. хотеть сказать: 16 шт.\n",
      "15. знать это: 15 шт.\n",
      "16. ответить ричард: 15 шт.\n",
      "17. друг друг: 14 шт.\n",
      "18. это мочь: 14 шт.\n",
      "19. это весь: 14 шт.\n",
      "20. несколько минута: 14 шт.\n",
      "21. машина время: 14 шт.\n",
      "22. электрический монах: 13 шт.\n",
      "23. человек который: 13 шт.\n",
      "24. дирк это: 13 шт.\n",
      "25. дирк джентлить: 13 шт.\n"
     ]
    }
   ],
   "source": [
    "top_x_ngrams(list_of_bigrams, 25, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342ccb9",
   "metadata": {},
   "source": [
    "Выводим топ триграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b795b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-25 3-грамм:\n",
      "1. это весь лишь: 8 шт.\n",
      "2. черта побрать это: 6 шт.\n",
      "3. дирк пожать плечо: 6 шт.\n",
      "4. детективный агентство дирк: 5 шт.\n",
      "5. фирма передовой технология: 5 шт.\n",
      "6. передовой технология уэя: 5 шт.\n",
      "7. холистический детективный агентство: 5 шт.\n",
      "8. агентство дирк джентлить: 5 шт.\n",
      "9. король георг третий: 4 шт.\n",
      "10. сэмюэль тейлор кольридж: 4 шт.\n",
      "11. поэма кубнуть хан: 4 шт.\n",
      "12. сказать леди магна: 4 шт.\n",
      "13. сказание старый мореход: 4 шт.\n",
      "14. это черта побрать: 3 шт.\n",
      "15. колледж святой седда: 3 шт.\n",
      "16. красный лыжный шапочка: 3 шт.\n",
      "17. спросить ричард профессор: 3 шт.\n",
      "18. мочь сделать это: 3 шт.\n",
      "19. компания передовой технология: 3 шт.\n",
      "20. сьюзан привет это: 3 шт.\n",
      "21. привет это гордон: 3 шт.\n",
      "22. стол красный дерево: 3 шт.\n",
      "23. иметь никакой отношение: 3 шт.\n",
      "24. крикнуть кухня профессор: 3 шт.\n",
      "25. это сделать ричард: 3 шт.\n"
     ]
    }
   ],
   "source": [
    "top_x_ngrams(list_of_trigrams, 25, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909fc83",
   "metadata": {},
   "source": [
    "Почему такие **биграммы**? Не удалились все стоп-слова, поэтому не все из них выглядят осмысленно для текста. Те, что выглядят - имена героев и названия организаций, которые упоминаются в тексте. Также в тексте много диалогов, фразы вроде \"спросить Ричард\" это - _спросил Ричард_, которые используются для оформления диалогической речи.\n",
    "\n",
    "**Триграммы**, в свою очередь, куда более специфичны для текста, в них в основном имена героев и заведений, их частые высказывания вроде _\"Чёрт побери эт(о/а/у)...!\"_ (к слову, криво лемматизированное к начальной форме _черта_) или частые действия: Дирк Джентли по сюжету часто совершает необдуманные поступки, не знает, как их объяснить, поэтому пожимает плечами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622db1f",
   "metadata": {},
   "source": [
    "## Задание 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb913a3a",
   "metadata": {},
   "source": [
    "Абзац из текста, с которым будем работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c97eafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Наконец он осторожно сел и стал смотреть телевизор.\n",
    "Вскоре машины полуночников разъехались по домам,\n",
    "за окном остались тишина и серый снег.\n",
    "Ему ничего не оставалось, как смириться.\n",
    "Заметив, что он слишком глубоко ушел в кресло\n",
    "и почти стал его составной частью, он неуклюже\n",
    "постарался выкарабкаться из него. Пытаясь развлечься,\n",
    "он влезал на стол и стоял на нем. Но это не улучшило его самочувствия.\n",
    "Оно становилось все хуже и перешло в отчаяние,\n",
    "а затем, очевидно, во что-то еще худшее.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5c53eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_words = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "08a5c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = [morph.parse(word)[0] for word in paragraph_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760ede5",
   "metadata": {},
   "source": [
    "А это код, где я взаимозаменяю числа слов, времена глаголов (для глаголов отдельно прописываю отсутствие настоящего времени у глаголов сов. вида и отсутствие лица для глаголов пр. вр.). Не будет идеально работать для всех текстов, но для этого фрагмента все глаголы в 3 лице - подходящий вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e3c199bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = []\n",
    "for el in parsed:\n",
    "    # меняем число существительных, глаголов и прилагательных\n",
    "    if el.tag.POS in ('NOUN', 'VERB', 'ADJF'):\n",
    "        if 'plur' in el.tag:\n",
    "            new_word = el.inflect({'sing'})\n",
    "        if 'sing' in el.tag:\n",
    "            new_word = el.inflect({'plur'})\n",
    "    # меняем время глаголов\n",
    "    if 'VERB' in el.tag:\n",
    "        if 'past' in new_word.tag.tense:\n",
    "            # обрабатываем случай с совершенным видом\n",
    "            if 'perf' in new_word.tag:\n",
    "                new_word = new_word.inflect({'futr', '3per'})\n",
    "            else:\n",
    "                new_word = new_word.inflect({'pres', '3per'})\n",
    "        elif 'pres' in new_word.tag.tense:\n",
    "            new_word = new_word.inflect({'past'})\n",
    "    # если слово - не сущ., гл. или прил.\n",
    "    else:\n",
    "        new_word = el\n",
    "    # добавляем в словарь, чтобы сформировать текст\n",
    "    new_text.append(new_word.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79db24a",
   "metadata": {},
   "source": [
    "Делаем первую букву текста заглавной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "97b5d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text[0] = new_text[0].capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf21fa8",
   "metadata": {},
   "source": [
    "Импортируем знаки препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c9f4f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a1ab0",
   "metadata": {},
   "source": [
    "Выводим измененный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9d31f6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наконец он осторожно сядут и станут смотреть телевизор. Вскоре машины полуночников разъедется по домам, за окном останется тишина и серый снег. Ему ничего не остаются, как смириться. Заметив, что он слишком глубоко уйдут в кресло и почти станут его составной частью, он неуклюже постараются выкарабкаться из него. Пытаясь развлечься, он влезают на стол и стоят на нем. Но это не улучшат его самочувствия. Оно становятся всё хуже и перейдут в отчаяние, а затем, очевидно, во что-то ещё худшее."
     ]
    }
   ],
   "source": [
    "print(new_text[0], end='')\n",
    "for i, word in enumerate(new_text[1:]):\n",
    "    if new_text[i] == '.':\n",
    "        word = word.capitalize()\n",
    "    if word not in pt:\n",
    "        print(' ' + word, end='')\n",
    "    else:\n",
    "        print(word, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
